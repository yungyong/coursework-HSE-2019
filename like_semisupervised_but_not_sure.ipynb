{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import artm\n",
    "import numpy as np\n",
    "from matplotlib.artist import getp\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# размеченные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_words = pd.read_csv('./df_with_words.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    819\n",
       "0.0    290\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_words['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# словарь в виде текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'clean_replace_ent_text_textrank_more_13'\n",
    "batch_folder = './{}_batches'.format(name)\n",
    "data_path = './{}_vw.txt'.format(name)\n",
    "dict_path = batch_folder + '/dictionary.dict'  \n",
    "\n",
    "dic = artm.Dictionary()\n",
    "\n",
    "if os.path.isfile(dict_path):\n",
    "    dic.load(dictionary_path=dict_path)\n",
    "\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=data_path, \n",
    "                                    data_format='vowpal_wabbit', \n",
    "                                    collection_name='nthhappens', \n",
    "                                    target_folder=batch_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic.save_text('./test_save_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test_save_text', 'r') as f:\n",
    "    res = f.read()\n",
    "\n",
    "res = res.split('\\n')\n",
    "\n",
    "dict_df = pd.DataFrame(list(map(lambda x: x.split(','), res[2:])), columns=res[1].split(',')).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# замена весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_1 = df_with_words[df_with_words['score'] == 1]['word'].values\n",
    "for_0 = df_with_words[df_with_words['score'] == 0]['word'].values\n",
    "\n",
    "for_1_mask = dict_df['token'].apply(lambda x: x in for_1)\n",
    "for_0_mask = dict_df['token'].apply(lambda x: x in for_0)\n",
    "\n",
    "dict_df.loc[for_1_mask, ' token_value'] = ' -1'\n",
    "dict_df.loc[for_0_mask, ' token_value'] = ' 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>class_id</th>\n",
       "      <th>token_value</th>\n",
       "      <th>token_tf</th>\n",
       "      <th>token_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasvalue</td>\n",
       "      <td>@default_class</td>\n",
       "      <td>9.091307333619625e-07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>childcontext</td>\n",
       "      <td>@default_class</td>\n",
       "      <td>9.091307333619625e-07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voicecommand</td>\n",
       "      <td>@default_class</td>\n",
       "      <td>9.091307333619625e-07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i_</td>\n",
       "      <td>@default_class</td>\n",
       "      <td>9.091307333619625e-07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ilumx</td>\n",
       "      <td>@default_class</td>\n",
       "      <td>9.091307333619625e-07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token         class_id             token_value  token_tf  token_df\n",
       "0      hasvalue   @default_class   9.091307333619625e-07       5.0       2.0\n",
       "1  childcontext   @default_class   9.091307333619625e-07       5.0       1.0\n",
       "2  voicecommand   @default_class   9.091307333619625e-07       5.0       3.0\n",
       "3            i_   @default_class   9.091307333619625e-07       5.0       1.0\n",
       "4         ilumx   @default_class   9.091307333619625e-07       5.0       1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# запись текстового словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./vocab_semisuper_file.txt', 'w') as f:\n",
    "    f.write(res[0])\n",
    "    f.write('\\n')\n",
    "    f.write(res[1])\n",
    "    f.write('\\n')\n",
    "    for _, row in dict_df.iterrows():\n",
    "        f.write(','.join(row.values))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    57332\n",
       "-1     1506\n",
       " 1      504\n",
       "Name:  token_value, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_df[' token_value'].apply(lambda x: round(float(x)), 4).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = artm.Dictionary()\n",
    "\n",
    "dictionary.load_text(dictionary_path='./vocab_semisuper_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# таким образом, есть 2 словаря dictionary (новый) и dic (старый)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regulars(class_ids, dictionary, num_collection_passes=30, num_document_passes=5, pmt=0.3, seed=0):\n",
    "    # initialization\n",
    "    model = artm.ARTM(topic_names=topic_names, cache_theta=True, seed=seed,\n",
    "                       class_ids=class_ids,\n",
    "                       scores=[artm.PerplexityScore(name='PerplexityScore',\n",
    "                                                    dictionary=dictionary)],\n",
    "                     regularizers=[artm.SmoothSparseThetaRegularizer(\n",
    "                         name='SparseTheta', tau=-0.5)])    \n",
    "    # scores\n",
    "    model.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore'))\n",
    "    model.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore'))\n",
    "    model.scores.add(artm.TopicKernelScore(name='TopicKernelScore', topic_names=topic_names,dictionary=dictionary,\n",
    "                                                  probability_mass_threshold=pmt, class_id='@default_class'))\n",
    "    model.scores.add(artm.TopTokensScore(name='TopTokensScore', num_tokens=30, \n",
    "                                         class_id='@default_class', dictionary=dictionary))\n",
    "    \n",
    "    model.scores.add(artm.SparsityPhiScore(name='SparsityScoreModal', class_id='@textrank'))\n",
    "    #regularizers\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(\n",
    "                        name='SparsePhi', tau=-0.1, class_ids='@default_class'))\n",
    "    model.regularizers.add(artm.DecorrelatorPhiRegularizer(\n",
    "                        name='DecorrelatorPhi', tau=1e+7, class_ids='@default_class'))\n",
    "    \n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(\n",
    "                        name='SPPhiTextrankReg', tau=-0.2, class_ids='@textrank'))\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(\n",
    "                        name='SPPhiEntsReg', tau=-0.2, class_ids='@ents'))\n",
    "    # dictionary\n",
    "    model.num_document_passes = num_document_passes\n",
    "    model.initialize(dictionary=dictionary)\n",
    "\n",
    "    #fit 1\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=num_collection_passes)\n",
    "    print('model fitted', ' ', datetime.datetime.now().time())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "topic_names = ['topic_{}'.format(i) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids={'@default_class': 1.0, '@ents': 3.0, '@textrank': 1.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitted   00:03:59.631308\n"
     ]
    }
   ],
   "source": [
    "model_seed_10_dic = train_regulars(class_ids, dic seed=10, num_collection_passes=50)\n",
    "model_seed_10_dictionary = train_regulars(class_ids, dictionary, seed=10, num_collection_passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary == dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seed_10_dic.get_phi().equals(model_seed_10_dictionary.get_phi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seed_10_dic == model_seed_10_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_tuned</th>\n",
       "      <th>topic_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[intended, flat, engines, categories, shall, officer, referred, stretch, administrative, please_dont, dpo, wash, free_software, tuning, dude, organisations, fitness, nontechnical, watches, data_subjects, infringement, youre_wrong, washing, unreliable, glibc, permitted, soap, tuned, entrenched, maintainers]</td>\n",
       "      <td>topic_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[unions, instagram, warnings, engagement, guilty, cynical, likelihood, casual, separation, vitamin, yup, hunting, tactics, systemic, rings, trials, posters, misinformation, jury, punished, pleasure, hollywood, gmo, worldview, helmet, significance, sister, gogs, marijuana, attitudes]</td>\n",
       "      <td>topic_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ton, tons, solved, interviews, detect, degrees, resolution, spec, foot, solving, solve_problem, feet, wheel, fiber, steam, tradeoff, electronics, angle, much_harder, whiteboard, metric, jump, velocity, interviewing, discrete, pipe, acceleration, min, interviewer, wheels]</td>\n",
       "      <td>topic_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[facebook, default, block, instance, essentially, secure, uber, click, requests, googles, info, button, delete, log, map, blog, track, blocking, identity, http, maps, logs, targeted, settings, gmail, blog_post, targeting, handle, cookies, ip_address]</td>\n",
       "      <td>topic_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[helpful, perform, resource, ruby, decentralized, fork, assumed, centralized, evaluate, assumes, controversial, nontrivial, cognitive, spotify, creator, performed, expressed, actor, artists, optimal, steve, seed, strikes, streams, wellknown, maths, mastodon, opaque, bob, distinct]</td>\n",
       "      <td>topic_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             top_tuned  \\\n",
       "0  [intended, flat, engines, categories, shall, officer, referred, stretch, administrative, please_dont, dpo, wash, free_software, tuning, dude, organisations, fitness, nontechnical, watches, data_subjects, infringement, youre_wrong, washing, unreliable, glibc, permitted, soap, tuned, entrenched, maintainers]   \n",
       "1                          [unions, instagram, warnings, engagement, guilty, cynical, likelihood, casual, separation, vitamin, yup, hunting, tactics, systemic, rings, trials, posters, misinformation, jury, punished, pleasure, hollywood, gmo, worldview, helmet, significance, sister, gogs, marijuana, attitudes]   \n",
       "2                                     [ton, tons, solved, interviews, detect, degrees, resolution, spec, foot, solving, solve_problem, feet, wheel, fiber, steam, tradeoff, electronics, angle, much_harder, whiteboard, metric, jump, velocity, interviewing, discrete, pipe, acceleration, min, interviewer, wheels]   \n",
       "3                                                           [facebook, default, block, instance, essentially, secure, uber, click, requests, googles, info, button, delete, log, map, blog, track, blocking, identity, http, maps, logs, targeted, settings, gmail, blog_post, targeting, handle, cookies, ip_address]   \n",
       "4                            [helpful, perform, resource, ruby, decentralized, fork, assumed, centralized, evaluate, assumes, controversial, nontrivial, cognitive, spotify, creator, performed, expressed, actor, artists, optimal, steve, seed, strikes, streams, wellknown, maths, mastodon, opaque, bob, distinct]   \n",
       "\n",
       "  topic_tuned  \n",
       "0     topic_0  \n",
       "1     topic_1  \n",
       "2     topic_2  \n",
       "3     topic_3  \n",
       "4     topic_4  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = model_seed_10_dic\n",
    "qwe = []\n",
    "for topic_name in fin.topic_names:\n",
    "    qwe.append({'topic_tuned': topic_name, 'top_tuned': fin.score_tracker['TopTokensScore'].last_tokens[topic_name]})\n",
    "\n",
    "pd.options.display.max_colwidth = 3000\n",
    "\n",
    "pd.DataFrame(qwe).iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_tuned</th>\n",
       "      <th>topic_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[intended, flat, engines, categories, referred, approved, mitigate, administrative, please_dont, dpo, wash, organisations, fitness, nontechnical, data_subjects, infringement, youre_wrong, washing, soap, markup, thrust, entrenched, friend_mine, original_comment, systematic, wtf, conflict_interest, crossed, marketed, borderline]</td>\n",
       "      <td>topic_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[union, naive, falling, unions, instagram, engagement, cynical, casual, bikes, hunting, wrt, systemic, collectively, trials, moreover, jury, punished, nonexistent, algorithmic, hollywood, criticizing, gmo, fallen, surrounded, helmet, stationary, people_never, crops, never_ever, anxiety]</td>\n",
       "      <td>topic_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[keeping, style, ton, tons, distance, surface, solved, maintenance, detect, degrees, unknown, solving, latency, metric, spec, foot, feet, fiber, steam, grade, electronics, electrical, whiteboard, stretch, velocity, ford, tuning, acceleration, distances, interviewer]</td>\n",
       "      <td>topic_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[default, block, instance, essentially, prevent, topic, track, secure, blog, blocking, blocks, brand, http, maps, attacks, settings, blog_post, targeting, ip_address, instances, blocked, pii, handle, vulnerable, ip_addresses, malicious, google_maps, flag, generating, prevents]</td>\n",
       "      <td>topic_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[perform, resource, consensus, ruby, decentralized, fork, centralized, cognitive, yc, controversial, nontrivial, streams, spotify, performing, performed, expressed, actor, director, artists, steve, highly_recommend, assign, wellknown, kafka, maths, radar, arts, mastodon, opaque, void]</td>\n",
       "      <td>topic_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[extra, domain, statement, light, head, object, algorithm, algorithms, git, domains, strange, register, id_love, array, constraints, qt, technique, branch, matrix, mathematical, heads, grab, session, finger, registered, clone, pocket, trick, tricks, simulation]</td>\n",
       "      <td>topic_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                  top_tuned  \\\n",
       "0  [intended, flat, engines, categories, referred, approved, mitigate, administrative, please_dont, dpo, wash, organisations, fitness, nontechnical, data_subjects, infringement, youre_wrong, washing, soap, markup, thrust, entrenched, friend_mine, original_comment, systematic, wtf, conflict_interest, crossed, marketed, borderline]   \n",
       "1                                           [union, naive, falling, unions, instagram, engagement, cynical, casual, bikes, hunting, wrt, systemic, collectively, trials, moreover, jury, punished, nonexistent, algorithmic, hollywood, criticizing, gmo, fallen, surrounded, helmet, stationary, people_never, crops, never_ever, anxiety]   \n",
       "2                                                                [keeping, style, ton, tons, distance, surface, solved, maintenance, detect, degrees, unknown, solving, latency, metric, spec, foot, feet, fiber, steam, grade, electronics, electrical, whiteboard, stretch, velocity, ford, tuning, acceleration, distances, interviewer]   \n",
       "3                                                     [default, block, instance, essentially, prevent, topic, track, secure, blog, blocking, blocks, brand, http, maps, attacks, settings, blog_post, targeting, ip_address, instances, blocked, pii, handle, vulnerable, ip_addresses, malicious, google_maps, flag, generating, prevents]   \n",
       "4                                             [perform, resource, consensus, ruby, decentralized, fork, centralized, cognitive, yc, controversial, nontrivial, streams, spotify, performing, performed, expressed, actor, director, artists, steve, highly_recommend, assign, wellknown, kafka, maths, radar, arts, mastodon, opaque, void]   \n",
       "5                                                                     [extra, domain, statement, light, head, object, algorithm, algorithms, git, domains, strange, register, id_love, array, constraints, qt, technique, branch, matrix, mathematical, heads, grab, session, finger, registered, clone, pocket, trick, tricks, simulation]   \n",
       "\n",
       "  topic_tuned  \n",
       "0     topic_0  \n",
       "1     topic_1  \n",
       "2     topic_2  \n",
       "3     topic_3  \n",
       "4     topic_4  \n",
       "5     topic_5  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = model_seed_10_dictionary\n",
    "qwe = []\n",
    "for topic_name in fin.topic_names:\n",
    "    qwe.append({'topic_tuned': topic_name, 'top_tuned': fin.score_tracker['TopTokensScore'].last_tokens[topic_name]})\n",
    "\n",
    "pd.options.display.max_colwidth = 3000\n",
    "\n",
    "pd.DataFrame(qwe).iloc[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_tuned</th>\n",
       "      <th>topic_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[momentum, certification, judicial, bac, boeing, antenna, consist, dui, nut, taking_account, supervisory, noncompliance, tuners, emdrive, appoint, photon, stay_home, heater, takeoff, xcode, gs, headphone, emitted, risk_management, gloves, disincentive, black_holes, curious_know, emission, carplay]</td>\n",
       "      <td>topic_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[workable, problem_isnt, harvest, selectively, tire, starving, helmets, gmos, orbits, travelers, monsanto, trickle, soup, amino, hawaii, bath, plea, armchair, deficiencies, ant, golden_rice, amazes, descriptor, xs, unarmed, conflicted, persists, presuming, surroundings, layman]</td>\n",
       "      <td>topic_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ball, whiteboard, reserve, nasa, measuring, cm, fractional, xx, hangouts, voltage, inches, apollo, margin_error, whiteboards, hackintosh, rscience, ssn, juggle, grams, increments, mg, distinctions, combustion, monolith, frontpage, bad_behavior, moores_law, open_door, disservice, unification]</td>\n",
       "      <td>topic_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[vm, https, debian, certificate, vms, firewall, fa, registry, command_line, contribution, reuse, right_way, ec, linux_kernel, gcp, eu_citizen, init, oni, vpns, identifier, replication, pods, cloud_providers, installs, natively, rdf, gvisor, google_amazon, chromeos, curl]</td>\n",
       "      <td>topic_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[recursion, monad, datalog, b_c, validity, duck, basket, forked, widgets, defer, nim, gsuite, billionaire, alan, digest, forks, dan, interpreters, nobel_prize, cons, trustless, omission, thiel, invariant, ripple, multithreading, graal, hawking, ryan, cryptographically]</td>\n",
       "      <td>topic_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                    top_tuned  \\\n",
       "0  [momentum, certification, judicial, bac, boeing, antenna, consist, dui, nut, taking_account, supervisory, noncompliance, tuners, emdrive, appoint, photon, stay_home, heater, takeoff, xcode, gs, headphone, emitted, risk_management, gloves, disincentive, black_holes, curious_know, emission, carplay]   \n",
       "1                      [workable, problem_isnt, harvest, selectively, tire, starving, helmets, gmos, orbits, travelers, monsanto, trickle, soup, amino, hawaii, bath, plea, armchair, deficiencies, ant, golden_rice, amazes, descriptor, xs, unarmed, conflicted, persists, presuming, surroundings, layman]   \n",
       "2       [ball, whiteboard, reserve, nasa, measuring, cm, fractional, xx, hangouts, voltage, inches, apollo, margin_error, whiteboards, hackintosh, rscience, ssn, juggle, grams, increments, mg, distinctions, combustion, monolith, frontpage, bad_behavior, moores_law, open_door, disservice, unification]   \n",
       "3                             [vm, https, debian, certificate, vms, firewall, fa, registry, command_line, contribution, reuse, right_way, ec, linux_kernel, gcp, eu_citizen, init, oni, vpns, identifier, replication, pods, cloud_providers, installs, natively, rdf, gvisor, google_amazon, chromeos, curl]   \n",
       "4                               [recursion, monad, datalog, b_c, validity, duck, basket, forked, widgets, defer, nim, gsuite, billionaire, alan, digest, forks, dan, interpreters, nobel_prize, cons, trustless, omission, thiel, invariant, ripple, multithreading, graal, hawking, ryan, cryptographically]   \n",
       "\n",
       "  topic_tuned  \n",
       "0     topic_0  \n",
       "1     topic_1  \n",
       "2     topic_2  \n",
       "3     topic_3  \n",
       "4     topic_4  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = model_seed_10_another_dict_plus_reg\n",
    "qwe = []\n",
    "for topic_name in fin.topic_names:\n",
    "    qwe.append({'topic_tuned': topic_name, 'top_tuned': fin.score_tracker['TopTokensScore'].last_tokens[topic_name]})\n",
    "\n",
    "pd.options.display.max_colwidth = 3000\n",
    "\n",
    "pd.DataFrame(qwe).iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regulars_both_dicts(class_ids, num_collection_passes=30, num_document_passes=5, pmt=0.3, seed=0):\n",
    "    # initialization\n",
    "    model = artm.ARTM(topic_names=topic_names, cache_theta=True, seed=seed,\n",
    "                       class_ids=class_ids,\n",
    "                       scores=[artm.PerplexityScore(name='PerplexityScore',\n",
    "                                                    dictionary=dic),\n",
    "                       regularizers=[artm.SmoothSparseThetaRegularizer(\n",
    "                         name='SparseTheta', tau=-0.5)]])    \n",
    "    # scores\n",
    "    model.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore'))\n",
    "    model.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore'))\n",
    "    model.scores.add(artm.TopicKernelScore(name='TopicKernelScore', topic_names=topic_names,dictionary=dic,\n",
    "                                                  probability_mass_threshold=pmt, class_id='@default_class'))\n",
    "    model.scores.add(artm.TopTokensScore(name='TopTokensScore', num_tokens=30, \n",
    "                                         class_id='@default_class', dictionary=dic))\n",
    "    \n",
    "    model.scores.add(artm.SparsityPhiScore(name='SparsityScoreModal', class_id='@textrank'))\n",
    "    #regularizers\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(\n",
    "                         name='SmoothSparsePhiRegularizer', dictionary=dictionary))\n",
    "        \n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(\n",
    "                        name='SparsePhi', tau=-0.1, class_ids='@default_class'))\n",
    "    model.regularizers.add(artm.DecorrelatorPhiRegularizer(\n",
    "                        name='DecorrelatorPhi', tau=1e+7, class_ids='@default_class'))\n",
    "    \n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(\n",
    "                        name='SPPhiTextrankReg', tau=-0.2, class_ids='@textrank'))\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(\n",
    "                        name='SPPhiEntsReg', tau=-0.2, class_ids='@ents'))\n",
    "    # dictionary\n",
    "    model.num_document_passes = num_document_passes\n",
    "    model.initialize(dictionary=dic)\n",
    "\n",
    "    #fit 1\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=num_collection_passes)\n",
    "    print('model fitted', ' ', datetime.datetime.now().time())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitted   00:34:39.312466\n"
     ]
    }
   ],
   "source": [
    "model_seed_10_both = train_regulars_both_dicts(class_ids, seed=10, num_collection_passes=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### пример из туториала"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model.regularizer.add(\n",
    "    artm.SmoothSparsePhiRegularizer(name='smooth_sparse_phi_regularizer', dictionary=my_dictionary)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seed_10_otkatis.score_tracker['PerplexityScore'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_seed_10, model_seed_10_2, model_seed_10_another_dict, model_seed_10_another_dict_plus_reg, model_seed_10_experiment, model_seed_10_otkatis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для печати значений метрик нескольких моделей\n",
    "def print_measures_models(models):\n",
    "    print('Sparsity Phi: {}'.format('    '.join([\n",
    "        str(model_artm.score_tracker['SparsityPhiScore'].last_value) for model_artm in models])))\n",
    "\n",
    "    print('Sparsity Theta: {}'.format('    '.join([\n",
    "        str(model_artm.score_tracker['SparsityThetaScore'].last_value) for model_artm in models])))\n",
    "\n",
    "    print('Kernel contrast: {}'.format('    '.join([\n",
    "        str(model_artm.score_tracker['TopicKernelScore'].last_average_contrast) for model_artm in models])))\n",
    "\n",
    "    print('Kernel purity: {}'.format('    '.join([\n",
    "        str(model_artm.score_tracker['TopicKernelScore'].last_average_purity) for model_artm in models])))\n",
    "    \n",
    "    print('Kernel size: {}'.format('    '.join([\n",
    "        str(model_artm.score_tracker['TopicKernelScore'].last_average_size) for model_artm in models])))\n",
    "\n",
    "    print('Perplexity: {}'.format('    '.join([\n",
    "        str(model_artm.score_tracker['PerplexityScore'].last_value) for model_artm in models])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>122000</th>\n",
       "      <th>122001</th>\n",
       "      <th>122002</th>\n",
       "      <th>122003</th>\n",
       "      <th>122004</th>\n",
       "      <th>122005</th>\n",
       "      <th>122006</th>\n",
       "      <th>122007</th>\n",
       "      <th>122008</th>\n",
       "      <th>122009</th>\n",
       "      <th>...</th>\n",
       "      <th>63990</th>\n",
       "      <th>63991</th>\n",
       "      <th>63992</th>\n",
       "      <th>63993</th>\n",
       "      <th>63994</th>\n",
       "      <th>63995</th>\n",
       "      <th>63996</th>\n",
       "      <th>63997</th>\n",
       "      <th>63998</th>\n",
       "      <th>63999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic_13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051753</td>\n",
       "      <td>0.02649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_26</th>\n",
       "      <td>0.205665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028832</td>\n",
       "      <td>0.054137</td>\n",
       "      <td>0.095847</td>\n",
       "      <td>0.066456</td>\n",
       "      <td>0.159681</td>\n",
       "      <td>0.123966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.088216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341466</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.039468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024580</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101725</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            122000    122001  122002    122003    122004    122005    122006  \\\n",
       "topic_13  0.000000  0.166709     0.0  0.000000  0.000000  0.191174  0.000000   \n",
       "topic_26  0.205665  0.000000     0.0  0.028832  0.054137  0.095847  0.066456   \n",
       "topic_31  0.000000  0.000000     0.0  0.000000  0.000000  0.341466  0.004767   \n",
       "topic_34  0.000000  0.089462     0.0  0.000000  0.000000  0.085991  0.000000   \n",
       "topic_39  0.000000  0.072272     0.0  0.000000  0.000000  0.285523  0.000000   \n",
       "\n",
       "            122007    122008    122009    ...     63990     63991    63992   \\\n",
       "topic_13  0.000000  0.000000  0.000000    ...        0.0  0.051753  0.02649   \n",
       "topic_26  0.159681  0.123966  0.000000    ...        0.0  0.000000  0.00000   \n",
       "topic_31  0.039468  0.000000  0.000000    ...        0.0  0.000000  0.00000   \n",
       "topic_34  0.000000  0.000000  0.000000    ...        0.0  0.024580  0.00000   \n",
       "topic_39  0.000000  0.000000  0.066676    ...        0.0  0.000000  0.00000   \n",
       "\n",
       "            63993     63994   63995   63996   63997     63998     63999   \n",
       "topic_13  0.000000  0.000000     0.0     0.0     0.0  0.000000  0.100174  \n",
       "topic_26  0.088216  0.000000     0.0     0.0     0.0  0.000000  0.102929  \n",
       "topic_31  0.000000  0.019521     0.0     0.0     0.0  0.000000  0.000000  \n",
       "topic_34  0.000000  0.000000     0.0     0.0     0.0  0.101725  0.000000  \n",
       "topic_39  0.000000  0.000000     0.0     0.0     0.0  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 139301 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seed_10.get_theta()[model_seed_10.get_theta()[122005]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Phi: 0.9751600623130798    0.9761093258857727    0.9525267481803894    0.9933470487594604    0.9913120865821838    0.9525267481803894\n",
      "Sparsity Theta: 0.7829350829124451    0.7902418375015259    0.7855408191680908    0.0    0.018507836386561394    0.7855413556098938\n",
      "Kernel contrast: 0.8609730005264282    0.8598587512969971    0.766845703125    0.9998452067375183    0.9004098773002625    0.7668805718421936\n",
      "Kernel purity: 0.5702741742134094    0.7859736680984497    0.8402527570724487    0.9999809861183167    0.9699714779853821    0.8402342200279236\n",
      "Kernel size: 4010.43994140625    4020.60009765625    926.1400146484375    304.3999938964844    1782.219970703125    926.0999755859375\n",
      "Perplexity: 1577.3531494140625    1057.6279296875    nan    nan    1042.97314453125    nan\n"
     ]
    }
   ],
   "source": [
    "print_measures_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5863.5107421875,\n",
       " 1244.486328125,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seed_10_another_dict_plus_reg.score_tracker['PerplexityScore'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_0     0.991726\n",
       "topic_1     0.992653\n",
       "topic_2     0.991591\n",
       "topic_3     0.987951\n",
       "topic_4     0.990344\n",
       "topic_5     0.987092\n",
       "topic_6     0.992299\n",
       "topic_7     0.989586\n",
       "topic_8     0.988271\n",
       "topic_9     0.992720\n",
       "topic_10    0.991321\n",
       "topic_11    0.990732\n",
       "topic_12    0.987193\n",
       "topic_13    0.991574\n",
       "topic_14    0.987698\n",
       "topic_15    0.991541\n",
       "topic_16    0.990041\n",
       "topic_17    0.989653\n",
       "topic_18    0.990816\n",
       "topic_19    0.989502\n",
       "topic_20    0.975869\n",
       "topic_21    0.991018\n",
       "topic_22    0.987142\n",
       "topic_23    0.990732\n",
       "topic_24    0.991861\n",
       "topic_25    0.990513\n",
       "topic_26    0.990361\n",
       "topic_27    0.989889\n",
       "topic_28    0.989805\n",
       "topic_29    0.988524\n",
       "topic_30    0.993748\n",
       "topic_31    0.992114\n",
       "topic_32    0.991878\n",
       "topic_33    0.986064\n",
       "topic_34    0.992569\n",
       "topic_35    0.990327\n",
       "topic_36    0.992029\n",
       "topic_37    0.988878\n",
       "topic_38    0.989923\n",
       "topic_39    0.990378\n",
       "topic_40    0.992333\n",
       "topic_41    0.990277\n",
       "topic_42    0.992686\n",
       "topic_43    0.990664\n",
       "topic_44    0.991001\n",
       "topic_45    0.990209\n",
       "topic_46    0.990934\n",
       "topic_47    0.990580\n",
       "topic_48    0.986215\n",
       "topic_49    0.992602\n",
       "dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_seed_10_another_dict_plus_reg.get_phi() == 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_0     3.000000\n",
       "topic_1     3.000000\n",
       "topic_2     3.000003\n",
       "topic_3     2.999996\n",
       "topic_4     3.000003\n",
       "topic_5     3.000005\n",
       "topic_6     2.999998\n",
       "topic_7     2.999998\n",
       "topic_8     2.999995\n",
       "topic_9     2.999998\n",
       "topic_10    2.999998\n",
       "topic_11    3.000004\n",
       "topic_12    3.000002\n",
       "topic_13    2.999996\n",
       "topic_14    3.000001\n",
       "topic_15    2.999994\n",
       "topic_16    2.999991\n",
       "topic_17    2.999995\n",
       "topic_18    3.000001\n",
       "topic_19    2.999997\n",
       "topic_20    3.000005\n",
       "topic_21    2.999999\n",
       "topic_22    2.999991\n",
       "topic_23    3.000000\n",
       "topic_24    2.999999\n",
       "topic_25    3.000002\n",
       "topic_26    2.999999\n",
       "topic_27    2.999997\n",
       "topic_28    3.000004\n",
       "topic_29    2.999995\n",
       "topic_30    2.999997\n",
       "topic_31    2.999997\n",
       "topic_32    3.000001\n",
       "topic_33    2.999998\n",
       "topic_34    3.000001\n",
       "topic_35    3.000000\n",
       "topic_36    3.000000\n",
       "topic_37    2.999990\n",
       "topic_38    2.999990\n",
       "topic_39    2.999998\n",
       "topic_40    3.000004\n",
       "topic_41    3.000000\n",
       "topic_42    2.999996\n",
       "topic_43    2.999991\n",
       "topic_44    3.000003\n",
       "topic_45    3.000000\n",
       "topic_46    3.000001\n",
       "topic_47    2.999997\n",
       "topic_48    2.999998\n",
       "topic_49    3.000003\n",
       "dtype: float32"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_seed_10_another_dict_plus_reg.get_phi()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>122000</th>\n",
       "      <th>122001</th>\n",
       "      <th>122002</th>\n",
       "      <th>122003</th>\n",
       "      <th>122004</th>\n",
       "      <th>122005</th>\n",
       "      <th>122006</th>\n",
       "      <th>122007</th>\n",
       "      <th>122008</th>\n",
       "      <th>122009</th>\n",
       "      <th>...</th>\n",
       "      <th>63990</th>\n",
       "      <th>63991</th>\n",
       "      <th>63992</th>\n",
       "      <th>63993</th>\n",
       "      <th>63994</th>\n",
       "      <th>63995</th>\n",
       "      <th>63996</th>\n",
       "      <th>63997</th>\n",
       "      <th>63998</th>\n",
       "      <th>63999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic_0</th>\n",
       "      <td>0.039677</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.063715</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.018227</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.012983</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013407</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.037597</td>\n",
       "      <td>0.015782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_1</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.022350</td>\n",
       "      <td>0.036382</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.087302</td>\n",
       "      <td>0.010103</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.016988</td>\n",
       "      <td>0.021071</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.052238</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_2</th>\n",
       "      <td>0.014089</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.009582</td>\n",
       "      <td>0.036276</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>0.038579</td>\n",
       "      <td>0.017887</td>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.011779</td>\n",
       "      <td>0.035913</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_3</th>\n",
       "      <td>0.015872</td>\n",
       "      <td>0.013599</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.013852</td>\n",
       "      <td>0.039818</td>\n",
       "      <td>0.029521</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.045356</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.012986</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.015783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_4</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.013150</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.017892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.016768</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_5</th>\n",
       "      <td>0.050603</td>\n",
       "      <td>0.024395</td>\n",
       "      <td>0.014806</td>\n",
       "      <td>0.015376</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.127136</td>\n",
       "      <td>0.014413</td>\n",
       "      <td>0.013565</td>\n",
       "      <td>0.013266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030094</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.041406</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.016811</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.043214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_6</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.021856</td>\n",
       "      <td>0.014768</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016817</td>\n",
       "      <td>0.029506</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.011484</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.014118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_7</th>\n",
       "      <td>0.039679</td>\n",
       "      <td>0.013599</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.013951</td>\n",
       "      <td>0.015928</td>\n",
       "      <td>0.014761</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015039</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.032533</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.015782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_8</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.017979</td>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.044924</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_9</th>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.009575</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010103</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015047</td>\n",
       "      <td>0.011518</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.011506</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.023537</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_10</th>\n",
       "      <td>0.015872</td>\n",
       "      <td>0.033998</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.027692</td>\n",
       "      <td>0.015928</td>\n",
       "      <td>0.014761</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.045512</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.039455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_11</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.017881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.040220</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.041772</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_12</th>\n",
       "      <td>0.015872</td>\n",
       "      <td>0.013599</td>\n",
       "      <td>0.015968</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.041464</td>\n",
       "      <td>0.015928</td>\n",
       "      <td>0.014761</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060150</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.015783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_13</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>0.009603</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.039680</td>\n",
       "      <td>0.010656</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019070</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>0.133363</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>0.076404</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.014652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_14</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>0.036178</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.046358</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015039</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.017979</td>\n",
       "      <td>0.094268</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_15</th>\n",
       "      <td>0.035210</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.032630</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.017881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.016738</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.060150</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_16</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.038301</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.014970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.011585</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.011448</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_17</th>\n",
       "      <td>0.035344</td>\n",
       "      <td>0.011913</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.014615</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.025482</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.033226</td>\n",
       "      <td>0.013273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.011637</td>\n",
       "      <td>0.017750</td>\n",
       "      <td>0.046972</td>\n",
       "      <td>0.017664</td>\n",
       "      <td>0.013351</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.017771</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>0.013945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_18</th>\n",
       "      <td>0.015871</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>0.039823</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.018225</td>\n",
       "      <td>0.051952</td>\n",
       "      <td>0.012983</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013407</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>0.037513</td>\n",
       "      <td>0.015782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_19</th>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.034001</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>0.019334</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.027649</td>\n",
       "      <td>0.015931</td>\n",
       "      <td>0.014759</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.018221</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013335</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.015781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_20</th>\n",
       "      <td>0.015871</td>\n",
       "      <td>0.013598</td>\n",
       "      <td>0.032040</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.027739</td>\n",
       "      <td>0.015931</td>\n",
       "      <td>0.051658</td>\n",
       "      <td>0.046925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038790</td>\n",
       "      <td>0.036829</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.036848</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.030944</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.047350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_21</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.042729</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.067532</td>\n",
       "      <td>0.021071</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_22</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.074833</td>\n",
       "      <td>0.036232</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.027754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.045725</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_23</th>\n",
       "      <td>0.014910</td>\n",
       "      <td>0.012761</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.011645</td>\n",
       "      <td>0.030015</td>\n",
       "      <td>0.013949</td>\n",
       "      <td>0.013353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.014034</td>\n",
       "      <td>0.032672</td>\n",
       "      <td>0.012144</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.012657</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015138</td>\n",
       "      <td>0.014759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_24</th>\n",
       "      <td>0.015872</td>\n",
       "      <td>0.013599</td>\n",
       "      <td>0.021533</td>\n",
       "      <td>0.048079</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.015112</td>\n",
       "      <td>0.015928</td>\n",
       "      <td>0.014761</td>\n",
       "      <td>0.013278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.024917</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.032467</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.015783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_25</th>\n",
       "      <td>0.014096</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.013118</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.066973</td>\n",
       "      <td>0.013255</td>\n",
       "      <td>0.044540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>0.016707</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_26</th>\n",
       "      <td>0.015871</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.013251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.032595</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.039455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_27</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.065964</td>\n",
       "      <td>0.084387</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.025302</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.020322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.044218</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.021071</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.076391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_28</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.021856</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.014495</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010174</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015041</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.058830</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.034768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_29</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.022316</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010105</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022826</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.059174</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.013895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_30</th>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.021841</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.023016</td>\n",
       "      <td>0.013988</td>\n",
       "      <td>0.020204</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.011515</td>\n",
       "      <td>0.043089</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013525</td>\n",
       "      <td>0.014709</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.032643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_31</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.012725</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.023964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.028737</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_32</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010614</td>\n",
       "      <td>0.013990</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015066</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.037583</td>\n",
       "      <td>0.014095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_33</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.077567</td>\n",
       "      <td>0.013447</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.020581</td>\n",
       "      <td>0.016419</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.015542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.012324</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.029332</td>\n",
       "      <td>0.028977</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.017101</td>\n",
       "      <td>0.013899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_34</th>\n",
       "      <td>0.014269</td>\n",
       "      <td>0.035138</td>\n",
       "      <td>0.014258</td>\n",
       "      <td>0.014921</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010423</td>\n",
       "      <td>0.017807</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.016498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.036144</td>\n",
       "      <td>0.011588</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.036283</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.042194</td>\n",
       "      <td>0.014083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_35</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.053843</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010125</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.029029</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.014714</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_36</th>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.029305</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010175</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.014034</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.029259</td>\n",
       "      <td>0.058324</td>\n",
       "      <td>0.011791</td>\n",
       "      <td>0.013534</td>\n",
       "      <td>0.013962</td>\n",
       "      <td>0.013478</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.015994</td>\n",
       "      <td>0.013934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_37</th>\n",
       "      <td>0.015871</td>\n",
       "      <td>0.013599</td>\n",
       "      <td>0.016782</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.014124</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.059040</td>\n",
       "      <td>0.013257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015059</td>\n",
       "      <td>0.021811</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.033537</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.015840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_38</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.020083</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.018997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015041</td>\n",
       "      <td>0.023084</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_39</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.029587</td>\n",
       "      <td>0.013516</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.013987</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.027753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.022990</td>\n",
       "      <td>0.032469</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.013899</td>\n",
       "      <td>0.015049</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_40</th>\n",
       "      <td>0.035212</td>\n",
       "      <td>0.011836</td>\n",
       "      <td>0.009572</td>\n",
       "      <td>0.036222</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.020204</td>\n",
       "      <td>0.013988</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.011496</td>\n",
       "      <td>0.017977</td>\n",
       "      <td>0.019478</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.011767</td>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.015039</td>\n",
       "      <td>0.013890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_41</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.025648</td>\n",
       "      <td>0.019193</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015045</td>\n",
       "      <td>0.039348</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.015735</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013337</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.013891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_42</th>\n",
       "      <td>0.014560</td>\n",
       "      <td>0.018479</td>\n",
       "      <td>0.014939</td>\n",
       "      <td>0.015628</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.010958</td>\n",
       "      <td>0.014708</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>0.033573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>0.015766</td>\n",
       "      <td>0.011841</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>0.014472</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015208</td>\n",
       "      <td>0.014390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_43</th>\n",
       "      <td>0.071395</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.015981</td>\n",
       "      <td>0.019320</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.158723</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.036889</td>\n",
       "      <td>0.053021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015039</td>\n",
       "      <td>0.036277</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.013335</td>\n",
       "      <td>0.013414</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.060157</td>\n",
       "      <td>0.015776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_44</th>\n",
       "      <td>0.015871</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>0.048302</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.015930</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>0.033116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.018231</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.012986</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.053622</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015039</td>\n",
       "      <td>0.039444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_45</th>\n",
       "      <td>0.039678</td>\n",
       "      <td>0.033998</td>\n",
       "      <td>0.016264</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.018577</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.015785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_46</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.020241</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.023132</td>\n",
       "      <td>0.017979</td>\n",
       "      <td>0.019473</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.013890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_47</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.021856</td>\n",
       "      <td>0.014402</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.013390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.043105</td>\n",
       "      <td>0.012533</td>\n",
       "      <td>0.013345</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.036849</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015041</td>\n",
       "      <td>0.014184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_48</th>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>0.019181</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.010118</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.059568</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.034753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_49</th>\n",
       "      <td>0.035212</td>\n",
       "      <td>0.011835</td>\n",
       "      <td>0.009570</td>\n",
       "      <td>0.036228</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.034962</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.011495</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>0.038407</td>\n",
       "      <td>0.017902</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.017977</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.034718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 139301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            122000    122001    122002    122003    122004    122005  \\\n",
       "topic_0   0.039677  0.013600  0.039632  0.019337  0.034247  0.015873   \n",
       "topic_1   0.014085  0.019011  0.022350  0.036382  0.034247  0.087302   \n",
       "topic_2   0.014089  0.011838  0.009582  0.036276  0.013699  0.015873   \n",
       "topic_3   0.015872  0.013599  0.015977  0.019339  0.013699  0.015873   \n",
       "topic_4   0.014085  0.011834  0.013150  0.014493  0.013699  0.015873   \n",
       "topic_5   0.050603  0.024395  0.014806  0.015376  0.027397  0.039683   \n",
       "topic_6   0.014085  0.021856  0.014768  0.014493  0.013699  0.015873   \n",
       "topic_7   0.039679  0.013599  0.016352  0.019339  0.013699  0.015873   \n",
       "topic_8   0.014085  0.011834  0.009569  0.014493  0.013699  0.015873   \n",
       "topic_9   0.035211  0.011834  0.009575  0.028986  0.013699  0.015873   \n",
       "topic_10  0.015872  0.033998  0.016036  0.019339  0.013699  0.015873   \n",
       "topic_11  0.014085  0.011834  0.013100  0.014493  0.034247  0.015873   \n",
       "topic_12  0.015872  0.013599  0.015968  0.019339  0.054795  0.015873   \n",
       "topic_13  0.014085  0.023678  0.009603  0.014493  0.013699  0.039680   \n",
       "topic_14  0.014085  0.011834  0.009712  0.036178  0.034247  0.031746   \n",
       "topic_15  0.035210  0.011834  0.032630  0.014493  0.013699  0.039683   \n",
       "topic_16  0.014085  0.011834  0.038301  0.014493  0.013699  0.015873   \n",
       "topic_17  0.035344  0.011913  0.024330  0.014615  0.013699  0.015873   \n",
       "topic_18  0.015871  0.013600  0.015969  0.019336  0.034247  0.015873   \n",
       "topic_19  0.015870  0.034001  0.015969  0.019334  0.013699  0.039683   \n",
       "topic_20  0.015871  0.013598  0.032040  0.019345  0.054795  0.015873   \n",
       "topic_21  0.014085  0.019011  0.042729  0.014493  0.013699  0.015873   \n",
       "topic_22  0.014085  0.011834  0.074833  0.036232  0.013699  0.015873   \n",
       "topic_23  0.014910  0.012761  0.012108  0.016528  0.013699  0.015873   \n",
       "topic_24  0.015872  0.013599  0.021533  0.048079  0.013699  0.015873   \n",
       "topic_25  0.014096  0.011846  0.013118  0.014520  0.054795  0.015873   \n",
       "topic_26  0.015871  0.013600  0.016420  0.019338  0.013699  0.015873   \n",
       "topic_27  0.014085  0.065964  0.084387  0.014493  0.013699  0.015873   \n",
       "topic_28  0.014085  0.021856  0.014634  0.014495  0.013699  0.015873   \n",
       "topic_29  0.014085  0.019011  0.022316  0.014493  0.013699  0.015873   \n",
       "topic_30  0.014086  0.021841  0.014283  0.014497  0.013699  0.015873   \n",
       "topic_31  0.014085  0.011834  0.012725  0.014493  0.013699  0.015873   \n",
       "topic_32  0.014085  0.011834  0.021097  0.014500  0.013699  0.015873   \n",
       "topic_33  0.014085  0.077567  0.013447  0.014493  0.013699  0.015873   \n",
       "topic_34  0.014269  0.035138  0.014258  0.014921  0.013699  0.015873   \n",
       "topic_35  0.014085  0.053843  0.014338  0.014493  0.034247  0.015873   \n",
       "topic_36  0.014110  0.013989  0.029305  0.014553  0.013699  0.015873   \n",
       "topic_37  0.015871  0.013599  0.016782  0.019338  0.013699  0.015873   \n",
       "topic_38  0.014085  0.017703  0.019358  0.014493  0.013699  0.015873   \n",
       "topic_39  0.014085  0.029587  0.013516  0.014493  0.013699  0.015873   \n",
       "topic_40  0.035212  0.011836  0.009572  0.036222  0.013699  0.015873   \n",
       "topic_41  0.014085  0.011840  0.013893  0.014497  0.013699  0.015873   \n",
       "topic_42  0.014560  0.018479  0.014939  0.015628  0.013699  0.039683   \n",
       "topic_43  0.071395  0.013596  0.015981  0.019320  0.034247  0.015873   \n",
       "topic_44  0.015871  0.013601  0.015969  0.048302  0.013699  0.015873   \n",
       "topic_45  0.039678  0.033998  0.016264  0.019411  0.034247  0.015873   \n",
       "topic_46  0.014085  0.011834  0.009619  0.014493  0.013699  0.015873   \n",
       "topic_47  0.014085  0.021856  0.014402  0.014493  0.013699  0.015873   \n",
       "topic_48  0.014085  0.029579  0.019181  0.014493  0.027397  0.015876   \n",
       "topic_49  0.035212  0.011835  0.009570  0.036228  0.013699  0.015873   \n",
       "\n",
       "            122006    122007    122008    122009    ...       63990   \\\n",
       "topic_0   0.013848  0.063715  0.014760  0.013246    ...     0.015038   \n",
       "topic_1   0.010103  0.021352  0.013245  0.020192    ...     0.015038   \n",
       "topic_2   0.010110  0.013991  0.013249  0.013245    ...     0.015063   \n",
       "topic_3   0.013852  0.039818  0.029521  0.013254    ...     0.015038   \n",
       "topic_4   0.010115  0.019192  0.013245  0.017892    ...     0.015073   \n",
       "topic_5   0.127136  0.014413  0.013565  0.013266    ...     0.030094   \n",
       "topic_6   0.010350  0.013986  0.020213  0.013245    ...     0.016817   \n",
       "topic_7   0.013951  0.015928  0.014761  0.026490    ...     0.015039   \n",
       "topic_8   0.010101  0.013986  0.013245  0.013245    ...     0.015038   \n",
       "topic_9   0.010103  0.034966  0.013245  0.013245    ...     0.015047   \n",
       "topic_10  0.027692  0.015928  0.014761  0.013250    ...     0.015038   \n",
       "topic_11  0.010101  0.019192  0.013245  0.017881    ...     0.037594   \n",
       "topic_12  0.041464  0.015928  0.014761  0.013271    ...     0.060150   \n",
       "topic_13  0.010656  0.013986  0.013246  0.013245    ...     0.019070   \n",
       "topic_14  0.010180  0.013986  0.046358  0.013245    ...     0.015039   \n",
       "topic_15  0.025252  0.019192  0.013245  0.017881    ...     0.037594   \n",
       "topic_16  0.010131  0.015903  0.013245  0.014970    ...     0.015054   \n",
       "topic_17  0.025482  0.014073  0.033226  0.013273    ...     0.030075   \n",
       "topic_18  0.013847  0.039823  0.014760  0.013247    ...     0.015038   \n",
       "topic_19  0.027649  0.015931  0.014759  0.013248    ...     0.015038   \n",
       "topic_20  0.027739  0.015931  0.051658  0.046925    ...     0.038790   \n",
       "topic_21  0.010101  0.021352  0.013245  0.020192    ...     0.037594   \n",
       "topic_22  0.010132  0.013986  0.013245  0.027754    ...     0.015038   \n",
       "topic_23  0.011645  0.030015  0.013949  0.013353    ...     0.037594   \n",
       "topic_24  0.015112  0.015928  0.014761  0.013278    ...     0.015038   \n",
       "topic_25  0.010121  0.066973  0.013255  0.044540    ...     0.015038   \n",
       "topic_26  0.013968  0.015929  0.036900  0.013251    ...     0.015038   \n",
       "topic_27  0.025302  0.021352  0.013245  0.020322    ...     0.015038   \n",
       "topic_28  0.010174  0.013986  0.020213  0.013248    ...     0.015041   \n",
       "topic_29  0.010105  0.021352  0.013245  0.020192    ...     0.022826   \n",
       "topic_30  0.023016  0.013988  0.020204  0.013258    ...     0.015038   \n",
       "topic_31  0.010101  0.013986  0.033113  0.023964    ...     0.015038   \n",
       "topic_32  0.010614  0.013990  0.013245  0.032026    ...     0.015066   \n",
       "topic_33  0.020581  0.016419  0.013245  0.015542    ...     0.015038   \n",
       "topic_34  0.010423  0.017807  0.013403  0.016498    ...     0.015038   \n",
       "topic_35  0.010125  0.013986  0.020213  0.013326    ...     0.015042   \n",
       "topic_36  0.010175  0.015128  0.014034  0.014294    ...     0.015040   \n",
       "topic_37  0.014124  0.015929  0.059040  0.013257    ...     0.015059   \n",
       "topic_38  0.010115  0.020083  0.013245  0.018997    ...     0.015041   \n",
       "topic_39  0.010101  0.013987  0.013245  0.027753    ...     0.015038   \n",
       "topic_40  0.020204  0.013988  0.013245  0.013246    ...     0.015038   \n",
       "topic_41  0.025648  0.019193  0.013245  0.044657    ...     0.015045   \n",
       "topic_42  0.010958  0.014708  0.017920  0.033573    ...     0.015038   \n",
       "topic_43  0.158723  0.015936  0.036889  0.053021    ...     0.015039   \n",
       "topic_44  0.013848  0.015930  0.014760  0.033116    ...     0.015040   \n",
       "topic_45  0.013930  0.015929  0.036901  0.013245    ...     0.015042   \n",
       "topic_46  0.020241  0.013986  0.013245  0.026490    ...     0.037594   \n",
       "topic_47  0.010330  0.013986  0.020213  0.013390    ...     0.015091   \n",
       "topic_48  0.010118  0.013986  0.052980  0.013347    ...     0.015038   \n",
       "topic_49  0.010101  0.034962  0.013245  0.033113    ...     0.037594   \n",
       "\n",
       "            63991     63992     63993     63994     63995     63996   \\\n",
       "topic_0   0.018227  0.012988  0.012983  0.013333  0.013334  0.013407   \n",
       "topic_1   0.011502  0.017707  0.011429  0.013333  0.016988  0.021071   \n",
       "topic_2   0.011519  0.017961  0.038579  0.017887  0.033410  0.011779   \n",
       "topic_3   0.045356  0.012987  0.012986  0.013333  0.013333  0.013405   \n",
       "topic_4   0.011569  0.012987  0.011552  0.013333  0.013333  0.016768   \n",
       "topic_5   0.018233  0.016488  0.035100  0.041406  0.013333  0.042459   \n",
       "topic_6   0.029506  0.017257  0.011484  0.013333  0.013333  0.039481   \n",
       "topic_7   0.018731  0.012987  0.032533  0.033333  0.013333  0.013412   \n",
       "topic_8   0.011494  0.017979  0.057377  0.017903  0.013333  0.011765   \n",
       "topic_9   0.011518  0.012987  0.011506  0.013334  0.013333  0.023537   \n",
       "topic_10  0.045512  0.032468  0.012984  0.013333  0.013333  0.013405   \n",
       "topic_11  0.040220  0.012987  0.011429  0.013333  0.013333  0.041772   \n",
       "topic_12  0.018229  0.012987  0.012984  0.046667  0.026667  0.013405   \n",
       "topic_13  0.012720  0.012991  0.012880  0.133363  0.013333  0.013214   \n",
       "topic_14  0.012263  0.017979  0.094268  0.017903  0.013333  0.011765   \n",
       "topic_15  0.011502  0.012987  0.011438  0.033288  0.013333  0.016738   \n",
       "topic_16  0.011585  0.012987  0.011448  0.013333  0.053333  0.013564   \n",
       "topic_17  0.011637  0.017750  0.046972  0.017664  0.013351  0.011845   \n",
       "topic_18  0.018225  0.051952  0.012983  0.013333  0.013334  0.013407   \n",
       "topic_19  0.018221  0.012989  0.012982  0.013333  0.013335  0.013408   \n",
       "topic_20  0.036829  0.012987  0.036848  0.013333  0.013333  0.030944   \n",
       "topic_21  0.011494  0.017707  0.011429  0.013333  0.067532  0.021071   \n",
       "topic_22  0.011594  0.012987  0.045725  0.013333  0.013333  0.011765   \n",
       "topic_23  0.014034  0.032672  0.012144  0.013333  0.013411  0.012657   \n",
       "topic_24  0.024917  0.032468  0.032467  0.013333  0.046667  0.013406   \n",
       "topic_25  0.011526  0.012987  0.011439  0.013333  0.053035  0.016707   \n",
       "topic_26  0.018868  0.012988  0.032595  0.013333  0.013334  0.013406   \n",
       "topic_27  0.011561  0.044218  0.011435  0.013333  0.042274  0.021071   \n",
       "topic_28  0.011706  0.017257  0.011434  0.033333  0.013333  0.058830   \n",
       "topic_29  0.011500  0.017707  0.011486  0.013333  0.016918  0.059174   \n",
       "topic_30  0.011515  0.043089  0.011446  0.013333  0.013525  0.014709   \n",
       "topic_31  0.028737  0.012987  0.011429  0.013333  0.013333  0.011765   \n",
       "topic_32  0.013460  0.012987  0.019469  0.013333  0.013333  0.011776   \n",
       "topic_33  0.012851  0.014664  0.012324  0.013333  0.029332  0.028977   \n",
       "topic_34  0.012000  0.036144  0.011588  0.013334  0.036283  0.015836   \n",
       "topic_35  0.029029  0.017257  0.011526  0.013333  0.013333  0.014714   \n",
       "topic_36  0.029259  0.058324  0.011791  0.013534  0.013962  0.013478   \n",
       "topic_37  0.021811  0.012987  0.014410  0.013333  0.013354  0.033537   \n",
       "topic_38  0.023084  0.016963  0.011468  0.033333  0.016386  0.019231   \n",
       "topic_39  0.022990  0.032469  0.011429  0.013333  0.013334  0.011765   \n",
       "topic_40  0.011496  0.017977  0.019478  0.017900  0.013334  0.011767   \n",
       "topic_41  0.039348  0.012991  0.015735  0.013333  0.013337  0.016747   \n",
       "topic_42  0.012867  0.015766  0.011841  0.013334  0.026893  0.014472   \n",
       "topic_43  0.036277  0.012988  0.012979  0.033333  0.013335  0.013414   \n",
       "topic_44  0.018231  0.012988  0.012986  0.013333  0.013334  0.053622   \n",
       "topic_45  0.018577  0.012987  0.012984  0.013333  0.013338  0.013408   \n",
       "topic_46  0.023132  0.017979  0.019473  0.017903  0.013333  0.011765   \n",
       "topic_47  0.012471  0.043105  0.012533  0.013345  0.033333  0.036849   \n",
       "topic_48  0.059568  0.012987  0.023807  0.033333  0.013333  0.011765   \n",
       "topic_49  0.011495  0.017978  0.038407  0.017902  0.033334  0.011765   \n",
       "\n",
       "            63997     63998     63999   \n",
       "topic_0   0.027778  0.037597  0.015782  \n",
       "topic_1   0.013889  0.052238  0.013889  \n",
       "topic_2   0.035913  0.015038  0.013894  \n",
       "topic_3   0.013889  0.015038  0.015783  \n",
       "topic_4   0.013889  0.015038  0.013894  \n",
       "topic_5   0.016811  0.015038  0.043214  \n",
       "topic_6   0.013889  0.015038  0.014118  \n",
       "topic_7   0.013889  0.015038  0.015782  \n",
       "topic_8   0.044924  0.015038  0.013889  \n",
       "topic_9   0.013889  0.015038  0.013889  \n",
       "topic_10  0.013889  0.015038  0.039455  \n",
       "topic_11  0.013889  0.015038  0.013889  \n",
       "topic_12  0.013889  0.015038  0.015783  \n",
       "topic_13  0.076404  0.015038  0.014652  \n",
       "topic_14  0.017978  0.015038  0.013922  \n",
       "topic_15  0.013889  0.060150  0.013889  \n",
       "topic_16  0.013889  0.015038  0.013901  \n",
       "topic_17  0.017771  0.015063  0.013945  \n",
       "topic_18  0.034688  0.037513  0.015782  \n",
       "topic_19  0.013889  0.015040  0.015781  \n",
       "topic_20  0.013889  0.015040  0.047350  \n",
       "topic_21  0.055556  0.020924  0.013889  \n",
       "topic_22  0.013889  0.015038  0.013890  \n",
       "topic_23  0.013889  0.015138  0.014759  \n",
       "topic_24  0.013889  0.015038  0.015783  \n",
       "topic_25  0.013889  0.015038  0.013901  \n",
       "topic_26  0.013889  0.015038  0.039455  \n",
       "topic_27  0.034722  0.020924  0.076391  \n",
       "topic_28  0.013889  0.015042  0.034768  \n",
       "topic_29  0.013889  0.020924  0.013895  \n",
       "topic_30  0.013889  0.015038  0.032643  \n",
       "topic_31  0.013889  0.015038  0.013889  \n",
       "topic_32  0.013889  0.037583  0.014095  \n",
       "topic_33  0.013889  0.017101  0.013899  \n",
       "topic_34  0.013913  0.042194  0.014083  \n",
       "topic_35  0.013889  0.015038  0.013907  \n",
       "topic_36  0.014045  0.015994  0.013934  \n",
       "topic_37  0.013889  0.015038  0.015840  \n",
       "topic_38  0.034722  0.019980  0.013889  \n",
       "topic_39  0.013899  0.015049  0.013889  \n",
       "topic_40  0.017975  0.015039  0.013890  \n",
       "topic_41  0.013889  0.015042  0.013891  \n",
       "topic_42  0.013889  0.015208  0.014390  \n",
       "topic_43  0.013889  0.060157  0.015776  \n",
       "topic_44  0.013889  0.015039  0.039444  \n",
       "topic_45  0.013889  0.015038  0.015785  \n",
       "topic_46  0.017978  0.015038  0.013890  \n",
       "topic_47  0.013889  0.015041  0.014184  \n",
       "topic_48  0.062500  0.015038  0.034753  \n",
       "topic_49  0.017977  0.015038  0.034718  \n",
       "\n",
       "[50 rows x 139301 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seed_10_another_dict_plus_reg.get_theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config {\n",
       "  topic_name: \"topic_0\"\n",
       "  topic_name: \"topic_1\"\n",
       "  topic_name: \"topic_2\"\n",
       "  topic_name: \"topic_3\"\n",
       "  topic_name: \"topic_4\"\n",
       "  topic_name: \"topic_5\"\n",
       "  topic_name: \"topic_6\"\n",
       "  topic_name: \"topic_7\"\n",
       "  topic_name: \"topic_8\"\n",
       "  topic_name: \"topic_9\"\n",
       "  topic_name: \"topic_10\"\n",
       "  topic_name: \"topic_11\"\n",
       "  topic_name: \"topic_12\"\n",
       "  topic_name: \"topic_13\"\n",
       "  topic_name: \"topic_14\"\n",
       "  topic_name: \"topic_15\"\n",
       "  topic_name: \"topic_16\"\n",
       "  topic_name: \"topic_17\"\n",
       "  topic_name: \"topic_18\"\n",
       "  topic_name: \"topic_19\"\n",
       "  topic_name: \"topic_20\"\n",
       "  topic_name: \"topic_21\"\n",
       "  topic_name: \"topic_22\"\n",
       "  topic_name: \"topic_23\"\n",
       "  topic_name: \"topic_24\"\n",
       "  topic_name: \"topic_25\"\n",
       "  topic_name: \"topic_26\"\n",
       "  topic_name: \"topic_27\"\n",
       "  topic_name: \"topic_28\"\n",
       "  topic_name: \"topic_29\"\n",
       "  topic_name: \"topic_30\"\n",
       "  topic_name: \"topic_31\"\n",
       "  topic_name: \"topic_32\"\n",
       "  topic_name: \"topic_33\"\n",
       "  topic_name: \"topic_34\"\n",
       "  topic_name: \"topic_35\"\n",
       "  topic_name: \"topic_36\"\n",
       "  topic_name: \"topic_37\"\n",
       "  topic_name: \"topic_38\"\n",
       "  topic_name: \"topic_39\"\n",
       "  topic_name: \"topic_40\"\n",
       "  topic_name: \"topic_41\"\n",
       "  topic_name: \"topic_42\"\n",
       "  topic_name: \"topic_43\"\n",
       "  topic_name: \"topic_44\"\n",
       "  topic_name: \"topic_45\"\n",
       "  topic_name: \"topic_46\"\n",
       "  topic_name: \"topic_47\"\n",
       "  topic_name: \"topic_48\"\n",
       "  topic_name: \"topic_49\"\n",
       "  class_id: \"@default_class\"\n",
       "  class_id: \"@ents\"\n",
       "  class_id: \"@textrank\"\n",
       "  class_weight: 1.0\n",
       "  class_weight: 3.0\n",
       "  class_weight: 1.5\n",
       "  score_config {\n",
       "    name: \"PerplexityScore\"\n",
       "    type: ScoreType_Perplexity\n",
       "    config: \"\\010\\001\\022$8e163f8f-2d06-41fd-bde5-916d5104efd3\"\n",
       "    model_name: \"pwt\"\n",
       "  }\n",
       "  score_config {\n",
       "    name: \"SparsityPhiScore\"\n",
       "    type: ScoreType_SparsityPhi\n",
       "    config: \"\"\n",
       "    model_name: \"pwt\"\n",
       "  }\n",
       "  score_config {\n",
       "    name: \"SparsityThetaScore\"\n",
       "    type: ScoreType_SparsityTheta\n",
       "    config: \"\"\n",
       "    model_name: \"pwt\"\n",
       "  }\n",
       "  score_config {\n",
       "    name: \"TopicKernelScore\"\n",
       "    type: ScoreType_TopicKernel\n",
       "    config: \"\\022\\016@default_class\\032\\007topic_0\\032\\007topic_1\\032\\007topic_2\\032\\007topic_3\\032\\007topic_4\\032\\007topic_5\\032\\007topic_6\\032\\007topic_7\\032\\007topic_8\\032\\007topic_9\\032\\010topic_10\\032\\010topic_11\\032\\010topic_12\\032\\010topic_13\\032\\010topic_14\\032\\010topic_15\\032\\010topic_16\\032\\010topic_17\\032\\010topic_18\\032\\010topic_19\\032\\010topic_20\\032\\010topic_21\\032\\010topic_22\\032\\010topic_23\\032\\010topic_24\\032\\010topic_25\\032\\010topic_26\\032\\010topic_27\\032\\010topic_28\\032\\010topic_29\\032\\010topic_30\\032\\010topic_31\\032\\010topic_32\\032\\010topic_33\\032\\010topic_34\\032\\010topic_35\\032\\010topic_36\\032\\010topic_37\\032\\010topic_38\\032\\010topic_39\\032\\010topic_40\\032\\010topic_41\\032\\010topic_42\\032\\010topic_43\\032\\010topic_44\\032\\010topic_45\\032\\010topic_46\\032\\010topic_47\\032\\010topic_48\\032\\010topic_49%\\232\\231\\231>*$8e163f8f-2d06-41fd-bde5-916d5104efd3\"\n",
       "    model_name: \"pwt\"\n",
       "  }\n",
       "  score_config {\n",
       "    name: \"TopTokensScore\"\n",
       "    type: ScoreType_TopTokens\n",
       "    config: \"\\010\\036\\022\\016@default_class\\\"$8e163f8f-2d06-41fd-bde5-916d5104efd3\"\n",
       "    model_name: \"pwt\"\n",
       "  }\n",
       "  score_config {\n",
       "    name: \"SparsityScoreModal\"\n",
       "    type: ScoreType_SparsityPhi\n",
       "    config: \"\\022\\t@textrank\"\n",
       "    model_name: \"pwt\"\n",
       "  }\n",
       "  score_config {\n",
       "    name: \"^^^ItemsProcessedScore^^^\"\n",
       "    type: ScoreType_ItemsProcessed\n",
       "    config: \"\"\n",
       "  }\n",
       "  regularizer_config {\n",
       "    name: \"SmoothSparseThetaRegularizer\"\n",
       "    type: RegularizerType_SmoothSparseTheta\n",
       "    config: \"\\032\\002\\010\\002\"\n",
       "    tau: 1.0\n",
       "  }\n",
       "  regularizer_config {\n",
       "    name: \"SparsePhi\"\n",
       "    type: RegularizerType_SmoothSparsePhi\n",
       "    config: \"\\022\\016@default_class\\\"\\002\\010\\002\"\n",
       "    tau: -0.10000000149011612\n",
       "  }\n",
       "  regularizer_config {\n",
       "    name: \"DecorrelatorPhi\"\n",
       "    type: RegularizerType_DecorrelatorPhi\n",
       "    config: \"\\022\\016@default_class\"\n",
       "    tau: 10000000.0\n",
       "  }\n",
       "  regularizer_config {\n",
       "    name: \"SPPhiTextrankReg\"\n",
       "    type: RegularizerType_SmoothSparsePhi\n",
       "    config: \"\\022\\t@textrank\\\"\\002\\010\\002\"\n",
       "    tau: -0.20000000298023224\n",
       "  }\n",
       "  regularizer_config {\n",
       "    name: \"SPPhiEntsReg\"\n",
       "    type: RegularizerType_SmoothSparsePhi\n",
       "    config: \"\\022\\005@ents\\\"\\002\\010\\002\"\n",
       "    tau: -0.20000000298023224\n",
       "  }\n",
       "  pwt_name: \"pwt\"\n",
       "  nwt_name: \"nwt\"\n",
       "  num_document_passes: 5\n",
       "  reuse_theta: false\n",
       "  cache_theta: true\n",
       "}\n",
       "regularizer {\n",
       "  name: \"DecorrelatorPhi\"\n",
       "  type: \"N4artm11regularizer15DecorrelatorPhiE\"\n",
       "}\n",
       "regularizer {\n",
       "  name: \"SPPhiEntsReg\"\n",
       "  type: \"N4artm11regularizer15SmoothSparsePhiE\"\n",
       "}\n",
       "regularizer {\n",
       "  name: \"SPPhiTextrankReg\"\n",
       "  type: \"N4artm11regularizer15SmoothSparsePhiE\"\n",
       "}\n",
       "regularizer {\n",
       "  name: \"SmoothSparseThetaRegularizer\"\n",
       "  type: \"N4artm11regularizer17SmoothSparseThetaE\"\n",
       "}\n",
       "regularizer {\n",
       "  name: \"SparsePhi\"\n",
       "  type: \"N4artm11regularizer15SmoothSparsePhiE\"\n",
       "}\n",
       "score {\n",
       "  name: \"PerplexityScore\"\n",
       "  type: \"N4artm5score10PerplexityE\"\n",
       "}\n",
       "score {\n",
       "  name: \"SparsityPhiScore\"\n",
       "  type: \"N4artm5score11SparsityPhiE\"\n",
       "}\n",
       "score {\n",
       "  name: \"SparsityScoreModal\"\n",
       "  type: \"N4artm5score11SparsityPhiE\"\n",
       "}\n",
       "score {\n",
       "  name: \"SparsityThetaScore\"\n",
       "  type: \"N4artm5score13SparsityThetaE\"\n",
       "}\n",
       "score {\n",
       "  name: \"TopTokensScore\"\n",
       "  type: \"N4artm5score9TopTokensE\"\n",
       "}\n",
       "score {\n",
       "  name: \"TopicKernelScore\"\n",
       "  type: \"N4artm5score11TopicKernelE\"\n",
       "}\n",
       "score {\n",
       "  name: \"^^^ItemsProcessedScore^^^\"\n",
       "  type: \"N4artm5score14ItemsProcessedE\"\n",
       "}\n",
       "dictionary {\n",
       "  name: \"8e163f8f-2d06-41fd-bde5-916d5104efd3\"\n",
       "  num_entries: 59342\n",
       "  byte_size: 14092912\n",
       "}\n",
       "dictionary {\n",
       "  name: \"97a167ac-9255-4274-972c-b46a400015b8\"\n",
       "  num_entries: 344870\n",
       "  byte_size: 94318706\n",
       "}\n",
       "dictionary {\n",
       "  name: \"c2d14783-d840-4f13-a8df-a6b24ab698de\"\n",
       "  num_entries: 344870\n",
       "  byte_size: 94318706\n",
       "}\n",
       "model {\n",
       "  name: \"nwt\"\n",
       "  type: \"N4artm4core14DensePhiMatrixE\"\n",
       "  num_topics: 50\n",
       "  num_tokens: 59342\n",
       "  byte_size: 20417176\n",
       "}\n",
       "model {\n",
       "  name: \"pwt\"\n",
       "  type: \"N4artm4core14DensePhiMatrixE\"\n",
       "  num_topics: 50\n",
       "  num_tokens: 59342\n",
       "  byte_size: 18975784\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"040d6bb8-d93c-491c-8d55-a98d47aaf951\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"04f8265a-f320-471d-9e6f-68caa933677b\"\n",
       "  byte_size: 66108\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"08b3391f-2a90-424a-9576-071800ae979d\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"0a38569a-6c09-43d8-bef3-0dbb6a199df3\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"0db51b63-67d7-4cac-bea3-772e7673994f\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"0e77029d-827c-4bb0-b270-048a3d5f30d1\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"0f4d8824-ed68-469a-af66-d3b3bb57c04e\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"10b09082-0ea5-4021-b92a-8beb2d15aa44\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"10b51b7d-42ba-4d01-b3b5-c986fa17a49a\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"173e2dac-6fba-48e3-9a88-68cb0b66dcd7\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"193c220c-b60e-415d-8db1-2904b24f96c4\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"1cae3359-a36b-4c35-a92b-b8665c08e771\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"1e17811a-cce5-4a09-93a2-700946c7b303\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"1f4f146e-e0c0-44be-839e-6d90f64d4ddb\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"23ccfaeb-283d-4b22-8852-d394a86c7133\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"2460de43-50fb-46b0-8a6a-d96c7e999a06\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"25090d0a-c08a-45cd-928e-6af00a317e26\"\n",
       "  byte_size: 216490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"258cd8ce-e130-4b5a-9a07-5e1ca3b9540f\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"25b75526-b665-42cc-aa59-cbae39c351f7\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"298bae01-7ba4-490d-8683-4d47fa883dff\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"2aefc8ca-47d6-4330-bac6-8d69f3c5f7d5\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"2ce859d6-7339-468c-ab31-def6525bf3e5\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"2ecf26c6-cdc1-4fcd-9a63-8f6262dda1a4\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"307e71e6-265d-421c-aadb-af92900a62cc\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"30b41b38-7bc2-4b21-958c-aff857ea127a\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"331fa0e6-916f-4b36-bcbd-685e4da5771c\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"333e4bf8-6eb4-43ea-bb2e-e8408e54481a\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"35386772-b59b-44c5-a2ad-14783bf53f8e\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"358af6ca-091c-4fdb-a7d6-87c7d2405d1f\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"3648913c-e858-4d8f-8e31-35a38ec675b2\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"3bfb7ee9-1ad4-44b4-828c-a3a7fffef3ed\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"3eccd5a8-3587-4d8e-814e-d131e6b577ac\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"3f5315af-9022-4ee5-931a-a5e39ab51d3b\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4009e561-b813-40c8-ba90-b7a8c0c2efaa\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"40f75a94-63cf-44ae-a411-2a794c53d4b8\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"43c33882-ddf7-434e-a8a4-aefe9ed69b93\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"448d205d-133e-442e-9057-60a94f04cfcb\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4543a357-19ce-4485-abad-40c5747deb3f\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"489965bf-12ee-4a3a-92a5-963b464623f9\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4b51263e-ec59-43ac-a224-da6e9a9efd5b\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4b8cc47c-35b4-4c80-9e6b-ff43932a4e0f\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4cab72c9-c867-4ddd-b289-deba736bbbd9\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4cceae98-8a42-4d0e-bcba-7a50d06d4cd5\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4d8f122f-1c05-4309-8e06-1374590f922c\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4ee762ba-4d30-4e93-9714-2b85982ba854\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4ef6f85c-2309-4852-b6a5-9119fe436b29\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"4f595cd9-e390-47a0-bbc7-3f3ae8ee5dd1\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"523b1ae2-18af-4048-b98b-e850843c4620\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"53adf40c-2e99-4f95-9212-fedde80ebd2a\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"569079b3-d9cd-4f01-a572-e8be2909a031\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"5a08ba60-aa60-4eaa-8f6b-b406c7fbb2af\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"5ada542d-bdca-4dc9-bf70-aea60a24da43\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"5cab1f56-e2ae-4ef7-8666-91cdce3ed763\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"5cbd1e2a-2d32-4802-b647-f797ec24789f\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"5ccf9a39-6d07-4a9b-ac89-ba2d45a37b9c\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"5d53b5cd-5f2e-45d3-8a40-2f6be92fbaa4\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"621c64df-fb82-46ec-bf5c-ac47fdd4933c\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"62609171-10cc-4839-95e0-f737f0f9fc4d\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"629d85d4-52e8-440f-8900-06db8580249a\"\n",
       "  byte_size: 216490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"63bdc2f1-6931-4ba6-a413-24d74a97a831\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"6483ca41-8106-450e-97e6-0167b5fc8338\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"64e9b5ae-db6c-4571-838e-e6fc860445bd\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"653d5500-ec4d-49a6-bdf3-a8efffa1b6a9\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"662a3f50-40d9-4849-9123-222865cb57b3\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"68a28e43-62bd-45bc-bfe6-364d9c7e34ce\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"70904ee9-89ef-4c4b-94bb-6fbb8db4d26f\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"70a605df-647d-4c70-a159-22625a8226ff\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"7469c797-f220-42a5-8e18-aa1b5935d9d4\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"757f2f0d-9440-4ce7-9370-f9424919d461\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"75e574ba-2bd1-4950-8b6e-e36095ba5adc\"\n",
       "  byte_size: 216490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"768a8ed6-c4a3-4352-82de-d79d394ccb3b\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"793dfaaf-65a6-4309-8b8e-b92d683e1262\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"7977dda5-23c4-40a4-9c06-adab87259d76\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"79b8d7a1-3940-4b88-86d8-870c2948718a\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"7a8d4ec5-dbb1-4621-8aa4-ada6bfd066bc\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"7b58abb5-c121-4635-9ff7-a23d8c51cc6f\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"7ed7c150-d458-4852-b3a9-8bc9a55f3092\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"814f3a1e-6a4c-4293-8a90-4b848ed0072d\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"84205f30-2f73-4236-98a8-dea9d5c66e9d\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"845ae674-b3c4-41be-b137-ba874405b7af\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"8548baf5-e328-44e3-877b-b53be6fbc46b\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"874632a5-77bf-4c3d-887c-dca09e2d6d61\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"878289e5-cf71-4cdc-a2d3-4bab5d63f7f9\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"87cbffb4-bed1-42a1-8757-26775052c291\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"88b3ef6e-78a7-44c8-b0d6-d0c82acde3db\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"8c9740c1-9cdf-414a-99a2-dcd0b804158c\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"8f68d404-54f7-4cdd-9ec1-13fcaf7351ef\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"93abe900-9ec0-436d-9008-b223177d9686\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"93ae5f72-3812-4113-9807-740c4bbe85cc\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"9d96c8f0-c6c4-4932-aafb-d77b2c7abb3e\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"9dd79145-3a53-4e2d-8d1b-a379cf978ef3\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"a15c0d05-9ab7-44d2-97db-3ccbb91eff73\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"a253ae7f-461f-4d12-b786-8f6ce5b7e855\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"a3514417-4496-4cf4-8c16-83ebdf6acd62\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"a78ea49a-af65-4a85-9434-a72d698ba051\"\n",
       "  byte_size: 217106\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"a7b2900b-ac73-4d1e-995f-7d35d4ef5c96\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"aa5d61e2-ad76-47c7-ac90-2c4105a54f6a\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"afb44224-5f74-4977-9348-2ff169dd8fad\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"b4c790fc-6e93-41ac-87a0-6dad2e82b36f\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"b57983b0-d9a2-4514-91d9-4b8249ff270c\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"b764ef5a-66bc-483d-800d-6f4e2dd5243f\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"b79167a6-4133-4000-ae76-32201e5ceb54\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"b7fb1ecc-22fb-419c-92ff-899296ec8220\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"b9cb96b0-c0d6-480a-86c6-c7e5c5ddef86\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"b9fa0791-5e80-4cac-8349-29c9ff184c02\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"bae9f286-61bc-468b-84d2-c0d5d1231b29\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"c079b6b6-969e-4387-9a66-83894977e99a\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"c0a04a33-9b14-4cab-81d7-1da06ef03c3f\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"c20cdf25-d93a-4730-b600-493e883df221\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"c2496a7e-3cf1-44f9-b8bc-a45930b21492\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"c253e633-2a7b-4182-9dd6-78aa43c65ccc\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"c564d334-06b8-4c5a-bbb4-a5ea6c2c3e34\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"c5ba5476-424c-4f58-b049-9aef136fe507\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"ca4534c6-1a32-4bd2-a686-888d9ea1b5b4\"\n",
       "  byte_size: 216490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"ca889ea4-97cc-48ac-8dda-dd02ecb42bae\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"cf6e6c25-0b33-4f01-a1c1-9f89345e9fee\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d0cfb49c-c255-4ffd-b3aa-f2486f6e33b2\"\n",
       "  byte_size: 214252\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d314e94e-f261-417a-94aa-583ba7b79530\"\n",
       "  byte_size: 215490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d42d9037-5a3c-45ed-ad12-e5a9e05ec87b\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d4cd2eda-60e2-44f7-ba6a-64c8ad4ab550\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d649fa11-10ef-4450-87d5-4ad8395b462a\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d7286d9e-68ef-46d1-9192-0d0a711ef4cd\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d78c521a-e3b2-48e6-b1b2-fc64a8f538a3\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d7d5758b-2b24-4076-b93d-6b0b6d26ad8f\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"d9f317a6-e914-4d8e-b037-2c9c66513c2b\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"dca81393-5e4c-4771-969e-ed86bdf37133\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"e25d5aad-43ba-462f-8229-00da885450ff\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"e4196b80-74bb-4330-b6b8-ad1087b5c101\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"e4895838-4563-44cc-a915-734aad9f3c3f\"\n",
       "  byte_size: 216490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"e686d9b0-036a-4394-85b4-42d9958d7890\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"e6d196fe-5def-4671-a4cf-2023072b6910\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"edf03f32-d89b-4aa5-8002-d348651ef986\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"f3e8ea8d-9743-47ef-8ec8-ad3c696028e2\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"f6be9828-1ce3-46c6-9860-0a338df075b0\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"faeef037-ce1c-4a03-b9a8-5fa00c2aa1fa\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"fb3e2211-cb53-4a60-9103-3511acc9aab0\"\n",
       "  byte_size: 216490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"fc923bb0-dad1-43b6-b46a-7d13122f359e\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"fd9fc9ed-26a8-4905-9d5e-e293b29b62b8\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"fdb2f43d-7a0a-496c-bce0-db22ba875b14\"\n",
       "  byte_size: 218490\n",
       "}\n",
       "cache_entry {\n",
       "  key: \"fe01d653-133a-4206-8e7e-86341c105f02\"\n",
       "  byte_size: 217490\n",
       "}\n",
       "processor_queue_size: 0\n",
       "num_processors: 12"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seed_10_another_dict_plus_reg.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37959.32421875,\n",
       " 2047.3466796875,\n",
       " 1756.7103271484375,\n",
       " 1372.012451171875,\n",
       " 1373.1568603515625,\n",
       " 1240.6375732421875,\n",
       " 1176.43115234375,\n",
       " 1147.5987548828125,\n",
       " 1127.3853759765625,\n",
       " 1114.9215087890625,\n",
       " 1104.9720458984375,\n",
       " 1097.9609375,\n",
       " 1092.1961669921875,\n",
       " 1087.893798828125,\n",
       " 1084.22021484375,\n",
       " 1081.2933349609375,\n",
       " 1078.6639404296875,\n",
       " 1076.503173828125,\n",
       " 1074.533203125,\n",
       " 1072.9180908203125,\n",
       " 1071.4608154296875,\n",
       " 1070.26904296875,\n",
       " 1069.1663818359375,\n",
       " 1068.2724609375,\n",
       " 1067.39697265625,\n",
       " 1066.673828125,\n",
       " 1065.91845703125,\n",
       " 1065.32373046875,\n",
       " 1064.664306640625,\n",
       " 1064.1866455078125,\n",
       " 1063.6236572265625,\n",
       " 1063.1927490234375,\n",
       " 1062.6983642578125,\n",
       " 1062.3052978515625,\n",
       " 1061.8583984375,\n",
       " 1061.5108642578125,\n",
       " 1061.115478515625,\n",
       " 1060.8018798828125,\n",
       " 1060.436279296875,\n",
       " 1060.1495361328125,\n",
       " 1059.8116455078125,\n",
       " 1059.5484619140625,\n",
       " 1059.2388916015625,\n",
       " 1058.999267578125,\n",
       " 1058.7135009765625,\n",
       " 1058.495849609375,\n",
       " 1058.2352294921875,\n",
       " 1058.043212890625,\n",
       " 1057.8062744140625,\n",
       " 1057.628662109375]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seed_10.score_tracker['PerplexityScore'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
