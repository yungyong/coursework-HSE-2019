{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_coref_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/programming_languages.txt') as f:\n",
    "    languages = f.read().split('\\n')\n",
    "\n",
    "with open(\"./data/neg_words.txt\", 'r', encoding = \"ISO-8859-1\") as f:\n",
    "    neg = list(map(lambda x: x.strip(), f.readlines()))\n",
    "with open(\"./data/pos_words.txt\", 'r', encoding = \"ISO-8859-1\") as f:\n",
    "    pos  = list(map(lambda x: x.strip(), f.readlines()))\n",
    "opinion_words = neg + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = pd.read_csv('./data/df_hni_2018_05.csv', low_memory=False)\n",
    "\n",
    "# read lists from csv\n",
    "for list_col in \\\n",
    "    ['text_sentences', 'text_words', 'title_words', 'lengths_of_text_words', 'lengths_of_title_words']:\n",
    "    fin_df[list_col] = fin_df[list_col].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_inapropriate_string = ''\n",
    "for lang_orig in languages:\n",
    "    big_inapropriate_string += ''.join(re.findall(pattern=re.compile(\"[\\W]\"), string=lang_orig))\n",
    "\n",
    "inapropriate_syms = sorted(list(set(big_inapropriate_string)))\n",
    "\n",
    "inapropriate_syms_dict = dict(zip(inapropriate_syms, \n",
    "['Whitespace', 'Exclamation', 'Sharp',  \n",
    " 'Apostrophe', 'Bracket', 'Bracket',   \n",
    " 'Star', 'Plus', 'Comma', 'Hyphen', \n",
    " 'Dot', 'Slash', 'Colon', \n",
    " 'Dog',  'Dash',  'Apostrophe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 'Whitespace',\n",
       " '!': 'Exclamation',\n",
       " '#': 'Sharp',\n",
       " \"'\": 'Apostrophe',\n",
       " '(': 'Bracket',\n",
       " ')': 'Bracket',\n",
       " '*': 'Star',\n",
       " '+': 'Plus',\n",
       " ',': 'Comma',\n",
       " '-': 'Hyphen',\n",
       " '.': 'Dot',\n",
       " '/': 'Slash',\n",
       " ':': 'Colon',\n",
       " '@': 'Dog',\n",
       " '–': 'Dash',\n",
       " '′': 'Apostrophe'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inapropriate_syms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages  = list(map(lambda x: x.strip(), languages))\n",
    "\n",
    "lang_new = []\n",
    "for lang_orig in languages:\n",
    "    \n",
    "    lang_new.append(\n",
    "        reduce(lambda x, kv: x.replace(*kv), inapropriate_syms_dict.items(), lang_orig)\n",
    "    )\n",
    "    \n",
    "languages_dict = dict(zip(languages, lang_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df['all_words'] = fin_df['text_words']  + fin_df['title_words']\n",
    "\n",
    "fin_df.loc[fin_df['text'].isnull(), 'text'] = ''\n",
    "\n",
    "fin_df.loc[fin_df['title'].isnull(), 'title'] = ''\n",
    "\n",
    "fin_df['full_text'] = fin_df['text'] + fin_df['title'] \n",
    "\n",
    "fin_df['full_text2'] = fin_df['full_text'].apply(lambda txt: preproc_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score_dict = dict(zip(neg, [-1 for i in range(len(neg))]))\n",
    "pos_score_dict = dict(zip(pos, [1 for i in range(len(pos))]))\n",
    "\n",
    "all_score_dict.update(pos_score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_text(text):\n",
    "    text = re.sub(r'<\\w+>', ' ', text)\n",
    "    text = reduce(lambda x, kv: x.replace(*kv), languages_dict.items(), text)\n",
    "    return text\n",
    "\n",
    "def replace_pronouns(text):\n",
    "    return nlp(text)._.coref_resolved\n",
    "\n",
    "def split_sentence(text):\n",
    "    '''\n",
    "    splits review into a list of sentences using nlp's sentence parser\n",
    "    '''\n",
    "    review = nlp(text)\n",
    "    bag_sentence = []\n",
    "    start = 0\n",
    "    for token in review:\n",
    "        if token.sent_start:\n",
    "            bag_sentence.append(review[start:(token.i-1)])\n",
    "            start = token.i\n",
    "        if token.i == len(review)-1:\n",
    "            bag_sentence.append(review[start:(token.i+1)])\n",
    "    return bag_sentence\n",
    "\n",
    "# Remove special characters using regex\n",
    "def remove_special_char(sentence):\n",
    "    return re.sub(r\"[^a-zA-Z0-9.',:;?]+\", ' ', sentence)\n",
    "\n",
    "def feature_sentiment(sentence):\n",
    "    '''\n",
    "    input: dictionary and sentence\n",
    "    function: appends dictionary with new features if the feature did not exist previously,\n",
    "              then updates sentiment to each of the new or existing features\n",
    "    output: updated dictionary\n",
    "    '''\n",
    "\n",
    "    sent_dict = Counter()\n",
    "    sentence = nlp(sentence)\n",
    "\n",
    "    for token in sentence:\n",
    "        if token.text not in opinion_words:\n",
    "            continue\n",
    "\n",
    "        if (token.dep_ == \"advmod\"):\n",
    "            continue\n",
    "        \n",
    "        sentiment = 1 if token.text in pos else -1\n",
    "        \n",
    "        if (token.dep_ == \"amod\"):\n",
    "            print(\n",
    "                'Token \"{token}\" dependency is AMOD, incrementing key \"{head_text}\" by {sentiment}'\\\n",
    "                .format(\n",
    "                    token=token.text,\n",
    "                    head_text=token.head.text,\n",
    "                    sentiment=sentiment\n",
    "                )\n",
    "            )\n",
    "            sent_dict[token.head.text] += sentiment\n",
    "            continue\n",
    "        \n",
    "        print('Processing token \"{token}\" children #1'.format(token=token.text))\n",
    "        for child in token.children:\n",
    "            if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                print('Child \"{child}\" dependency is {dep} and child is OPINION WORD'\\\n",
    "                    .format(\n",
    "                        child=child.text,\n",
    "                        dep=child.dep_.upper()\n",
    "                    )\n",
    "                )\n",
    "                print('Multiplying sentiment by 1.5')\n",
    "                sentiment *= 1.5\n",
    "            elif child.dep_ == \"neg\":\n",
    "                print('Child \"{child}\" dependency is {dep}'\\\n",
    "                    .format(\n",
    "                        child=child.text,\n",
    "                        dep=child.dep_.upper()\n",
    "                    )\n",
    "                )\n",
    "                print('Multiplying sentiment by -1')\n",
    "                sentiment *= -1\n",
    "        \n",
    "        print('Processing token \"{token}\" children #2'.format(token=token.text))\n",
    "        for child in token.children:\n",
    "            if (token.pos_ == \"VERB\") and (child.dep_ == \"dobj\"):   \n",
    "                print('Token is a VERB and child \"{child}\" dependency is DOBJ, incrementing key \"{child}\" by {sentiment}'\\\n",
    "                    .format(\n",
    "                        child=child.text,\n",
    "                        sentiment=sentiment\n",
    "                    )\n",
    "                )\n",
    "                sent_dict[child.text] += sentiment\n",
    "                \n",
    "                # check for conjugates (a AND b), then add both to dictionary\n",
    "                subchildren = []\n",
    "                conj = False\n",
    "                \n",
    "                for subchild in child.children:\n",
    "                    if subchild.text == \"and\":\n",
    "                        conj = True\n",
    "                    \n",
    "                    if conj and (subchild.text != \"and\"):\n",
    "                        print('Found \"and {sub}\" conjurgate of {child}'.format(\n",
    "                            sub=subchild.text,\n",
    "                            child=child.text\n",
    "                        ))\n",
    "                        subchildren.append(subchild.text)\n",
    "                        conj = False\n",
    "                \n",
    "                for subchild in subchildren:\n",
    "                    print('Incrementing key \"{sub}\" by {sentiment}'.format(\n",
    "                        sub=subchild.text,\n",
    "                        sentiment=sentiment\n",
    "                    ))\n",
    "                    sent_dict[subchild] += sentiment\n",
    "    \n",
    "        # check for negation\n",
    "        print('Processing tokens \"{token}\" head \"{head}\" children #1'.format(\n",
    "            token=token.text,\n",
    "            head=token.head.text\n",
    "        ))\n",
    "        for child in token.head.children:\n",
    "            if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                sentiment *= 1.5\n",
    "            elif (child.dep_ == \"neg\"):\n",
    "                sentiment *= -1\n",
    "\n",
    "        # check for negation\n",
    "        print('Processing tokens \"{token}\" head \"{head}\" children #2'.format(\n",
    "            token=token.text,\n",
    "            head=token.head.text\n",
    "        ))\n",
    "        print(list(map(lambda x: x.text, token.head.children)))\n",
    "        for child in token.head.children:\n",
    "            noun = \"\"\n",
    "\n",
    "            if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\n",
    "                noun = child.text\n",
    "                \n",
    "                for subchild in child.children:\n",
    "                    if subchild.dep_ == \"compound\":\n",
    "                        noun = subchild.text + \" \" + noun\n",
    "                \n",
    "                print('Incrementing key \"{noun}\" by {sentiment}'.format(\n",
    "                    noun=noun,\n",
    "                    sentiment=sentiment\n",
    "                ))\n",
    "                sent_dict[noun] += sentiment                    \n",
    "                    \n",
    "    print(sent_dict)\n",
    "    return sent_dict\n",
    "\n",
    "def sentiment_pipe(text):\n",
    "    text2 = preproc_text(text)\n",
    "    review = replace_pronouns(text2)\n",
    "    sentences = split_sentence(review)\n",
    "    terms_dict = dict()\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_special_char(str(sentence))\n",
    "        terms_dict.update(feature_sentiment(str(sentence)))\n",
    "    return terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"C++ is a powerful and fast language. I like it.\"\"\"\n",
    "text2 = \"\"\"What is wrong with C++? Is it a powerful and fast language at all? I don't like it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans1 = sentiment_pipe(text1)\n",
    "ans2 = sentiment_pipe(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'CPlusPlus': 1, 'language': 1}, {'What': -1, 'it': -1, 'language': 1})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans1, ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fin_df['sentiments'] = fin_df['full_text'].apply(lambda x:  sentiment_pipe(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_sent_dict = dict()\n",
    "\n",
    "for local_sent_dict in fin_df['sentiments']:\n",
    "    final_sent_dict.update(local_sent_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
